{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path.cwd().joinpath(\"Data\", \"mpr_data_merged\")\n",
    "output_path = os.path.join(os.path.expanduser(\"~\"), \"documents/queensma/ma_essay/data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## SKLEARN ##\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "## TENSORFLOW ##\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Data was uploaded to kaggle and loaded into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>mpr</th>\n",
       "      <th>wordcount</th>\n",
       "      <th>NPositiveWords</th>\n",
       "      <th>NNegativeWords</th>\n",
       "      <th>NNeutralWords</th>\n",
       "      <th>NUncertainWords</th>\n",
       "      <th>NStrongWords</th>\n",
       "      <th>NWeakWords</th>\n",
       "      <th>...</th>\n",
       "      <th>Poswords</th>\n",
       "      <th>Negwords</th>\n",
       "      <th>Neuwords</th>\n",
       "      <th>Unwords</th>\n",
       "      <th>Strongwords</th>\n",
       "      <th>Weakwords</th>\n",
       "      <th>Conwords</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Information received since the last Monetary P...</td>\n",
       "      <td>2</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>stronger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>information received since the last monetary p...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>Information received since the last Monetary P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With the further strengthening of global deman...</td>\n",
       "      <td>2</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>strengthening</td>\n",
       "      <td>NaN</td>\n",
       "      <td>with the further strengthening of global deman...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>With the further strengthening of global deman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This has been particularly true for oil, lumbe...</td>\n",
       "      <td>2</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this has been particularly true for oil lumber...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>constraints</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>This has been particularly true for oil, lumbe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Higher crude oil prices have led to higher ene...</td>\n",
       "      <td>0</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>higher crude oil prices have led to higher ene...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>Higher crude oil prices have led to higher ene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As yet, however, these countries have not seen...</td>\n",
       "      <td>1</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>as yet however these countries have not seen a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>As yet, however, these countries have not seen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  class     mpr  \\\n",
       "0  Information received since the last Monetary P...      2  2000Q1   \n",
       "1  With the further strengthening of global deman...      2  2000Q1   \n",
       "2  This has been particularly true for oil, lumbe...      2  2000Q1   \n",
       "3  Higher crude oil prices have led to higher ene...      0  2000Q1   \n",
       "4  As yet, however, these countries have not seen...      1  2000Q1   \n",
       "\n",
       "   wordcount  NPositiveWords  NNegativeWords  NNeutralWords  NUncertainWords  \\\n",
       "0         41               1               0             41                0   \n",
       "1         25               1               0             25                0   \n",
       "2         18               0               0             17                0   \n",
       "3         25               0               0             25                0   \n",
       "4         26               0               0             26                0   \n",
       "\n",
       "   NStrongWords  NWeakWords  ...       Poswords Negwords  \\\n",
       "0             0           0  ...       stronger      NaN   \n",
       "1             0           0  ...  strengthening      NaN   \n",
       "2             0           0  ...            NaN      NaN   \n",
       "3             0           0  ...            NaN      NaN   \n",
       "4             0           0  ...            NaN      NaN   \n",
       "\n",
       "                                            Neuwords Unwords Strongwords  \\\n",
       "0  information received since the last monetary p...     NaN         NaN   \n",
       "1  with the further strengthening of global deman...     NaN         NaN   \n",
       "2  this has been particularly true for oil lumber...     NaN         NaN   \n",
       "3  higher crude oil prices have led to higher ene...     NaN         NaN   \n",
       "4  as yet however these countries have not seen a...     NaN         NaN   \n",
       "\n",
       "  Weakwords     Conwords quarter  year  \\\n",
       "0       NaN          NaN       1  2000   \n",
       "1       NaN          NaN       1  2000   \n",
       "2       NaN  constraints       1  2000   \n",
       "3       NaN          NaN       1  2000   \n",
       "4       NaN          NaN       1  2000   \n",
       "\n",
       "                                            raw_text  \n",
       "0  Information received since the last Monetary P...  \n",
       "1  With the further strengthening of global deman...  \n",
       "2  This has been particularly true for oil, lumbe...  \n",
       "3  Higher crude oil prices have led to higher ene...  \n",
       "4  As yet, however, these countries have not seen...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path.joinpath('data_deep_model_ready_V3.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the variables we want to use in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df['class']\n",
    "X = df[\n",
    "    ['text', \n",
    "     'year',\n",
    "     'quarter',\n",
    "     'wordcount', \n",
    "     'NPositiveWords', \n",
    "     'NNegativeWords', \n",
    "     'NUncertainWords', \n",
    "     'NConstWords', \n",
    "     'NStrongWords',\n",
    "     'NWeakWords']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-22 17:42:45.290134: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "## TOKENIZE AND VECTORIZE TEXT ##\n",
    "vectorizer = layers.TextVectorization()\n",
    "vectorizer.adapt(X['text'])\n",
    "\n",
    "# Fit normalizer to training data\n",
    "normalize = layers.Normalization()\n",
    "normalize.adapt(X.loc[:, 'year':])\n",
    "\n",
    "## SPLIT DATA ##\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESHAPE ##\n",
    "\"\"\"\n",
    "Unlike sklearn, kera's requires the class labels to be one-hot encoded.\n",
    "Therefore, we need to reshape the data into wide format\n",
    "\"\"\"\n",
    "\n",
    "def reshape_labels(data):\n",
    "    data = pd.DataFrame(data)\n",
    "    data[0] = data['class'].apply(lambda x: 1 if x == 0 else 0)\n",
    "    data[1] = data['class'].apply(lambda x: 1 if x == 1 else 0)\n",
    "    data[2] = data['class'].apply(lambda x: 1 if x == 2 else 0)\n",
    "    return data.loc[:, 0:]\n",
    "\n",
    "# Reshape target vectors\n",
    "y_train_wide = reshape_labels(y_train)\n",
    "y_test_wide = reshape_labels(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM -- Untrained EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uncompiled_model(\n",
    "    output_dim=64, \n",
    "    spatial_dropout_rate=0.3, \n",
    "    lstm_units=[128, 64, 32], \n",
    "    dense_units=[64, 32, 16], \n",
    "    dropout_rates=[0.2, 0.2, 0.2],\n",
    "    pre_trained_embedding=None\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Define the network/model\n",
    "    \n",
    "    This will be a 2 input model: \n",
    "        (i) text data \n",
    "        (ii) numeric data\n",
    "        \n",
    "    A 3 layer LSTM will be used to learn the text\n",
    "    A 3 layer MLP model for numeric features\n",
    "    \n",
    "    Text data will be vectorized during training \n",
    "    Numeric data will be normalized during training\n",
    "    \n",
    "    Both vectorization and normalization has been learned outside of the model\n",
    "    \"\"\"\n",
    "    # Define Network -- input layers\n",
    "    text_inputs = keras.Input(shape=(1, ), name='text', dtype=tf.string) # text\n",
    "    num_inputs = keras.Input(shape=(n_num_features, ), name='num') # numeric\n",
    "    \n",
    "    # Text layers\n",
    "    \"\"\"\n",
    "    Embedding layer, \n",
    "    Spatial dropout, \n",
    "    3 alternating LSTM and Batch Normalization layers\n",
    "    \n",
    "    NOTE: If a pre-trained embedding matrix is used, then the embedding layer\n",
    "    will not longer be trainable and will use the pre-trained embedding matrix given.\n",
    "    \"\"\"\n",
    "    if pre_trained_embedding is not None:\n",
    "        text_layers = layers.Embedding(\n",
    "            input_dim=vocab_size, \n",
    "            output_dim=output_dim,\n",
    "            embeddings_initializer=pre_trained_embedding, \n",
    "            trainable=False\n",
    "        )(vectorizer(text_inputs))\n",
    "    else:\n",
    "        text_layers = layers.Embedding(input_dim=vocab_size, output_dim=output_dim)(vectorizer(text_inputs))\n",
    "    \n",
    "    text_layers = layers.SpatialDropout1D(spatial_dropout_rate)(text_layers)\n",
    "    \n",
    "    text_layers = layers.LSTM(lstm_units[0], return_sequences=True)(text_layers)\n",
    "    text_layers = layers.BatchNormalization()(text_layers)\n",
    "    \n",
    "    text_layers = layers.LSTM(lstm_units[1], return_sequences=True)(text_layers)\n",
    "    text_layers = layers.BatchNormalization()(text_layers)\n",
    "    \n",
    "    text_layers = layers.LSTM(lstm_units[2])(text_layers)\n",
    "    text_layers = layers.BatchNormalization()(text_layers)\n",
    "\n",
    "    # Numeric layers\n",
    "    \"\"\"\n",
    "    Normalized numerical input data,\n",
    "    3 Dense layers with Dropout and Batch Normalization.\n",
    "    \n",
    "    Normalization is known to help neural networks.\n",
    "    \"\"\"\n",
    "    num_layers = layers.Dropout(dropout_rates[0])(normalize(num_inputs)) # normalize input data\n",
    "    num_layers = layers.Dense(dense_units[0], activation='relu')(num_layers)\n",
    "    num_layers = layers.BatchNormalization()(num_layers)\n",
    "\n",
    "    num_layers = layers.Dropout(dropout_rates[1])(num_layers)\n",
    "    num_layers = layers.Dense(dense_units[1], activation='relu')(num_layers)\n",
    "    num_layers = layers.BatchNormalization()(num_layers)\n",
    "\n",
    "    num_layers = layers.Dropout(dropout_rates[2])(num_layers)\n",
    "    num_layers = layers.Dense(dense_units[2], activation='relu')(num_layers)\n",
    "    num_layers = layers.BatchNormalization()(num_layers)\n",
    "    \n",
    "    # merge text and numeric features together\n",
    "    x = layers.concatenate([text_layers, num_layers])\n",
    "    \n",
    "    # Output layer\n",
    "    \"\"\"\n",
    "    Output layer, layer determines the form of the models output\n",
    "    A softmax activation function is used because of the multiclass model\n",
    "    3 unit dense layer is used because there are 3 classes (pos, neu, neg)\n",
    "    \"\"\"\n",
    "    outputs = layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "    # Model object\n",
    "    model = keras.Model(inputs=[text_inputs, num_inputs], outputs=outputs, name='LSTM')\n",
    "    return model\n",
    "\n",
    "def compile_model(model, learning_rate=0.001, use_ema=False, momentum=0.99):\n",
    "    \"\"\"\n",
    "    This compiles the model -- necessary step for all keras models\n",
    "    \n",
    "    Setup the loss/objective function\n",
    "    Choose optimization algorithm\n",
    "    Choose metrics used to measure loss or model performance\n",
    "    \"\"\"\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate, \n",
    "            use_ema=use_ema, \n",
    "            ema_momentum=momentum\n",
    "        ), \n",
    "        metrics=[\n",
    "            keras.metrics.CategoricalAccuracy(),\n",
    "            keras.metrics.AUC(multi_label=True, num_labels=3),\n",
    "            keras.metrics.Precision(),\n",
    "            keras.metrics.Recall()\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS TO GET PREDICTIONS AND METRICS\n",
    "# to use sklearn's classification report we need predicted labels as an output.\n",
    "\n",
    "def get_predicted_probs(model, x={\"text\": X_test.loc[:, 'text'], \"num\": X_test.loc[:, 'year':]}):\n",
    "    \"\"\"\n",
    "    We feed in the `text` and `num` data as seperate inputs.\n",
    "    \n",
    "    This gives us an array of the predicted probabilities for each class/label.\n",
    "    \n",
    "    e.g. [0.41, 0.89, 0.12]\n",
    "    \"\"\"\n",
    "    return model.predict(x)\n",
    "\n",
    "def get_predicted_labels(pred_probs):\n",
    "    \"\"\"\n",
    "    After obtaining the predicted probabilities, we can get the\n",
    "    predicted label using the `argmax` function -- base method use by kera's.\n",
    "    \"\"\"\n",
    "    pred_labels = []\n",
    "    \n",
    "    for i in range(len(pred_probs)):\n",
    "        pred_labels.append(np.argmax(pred_probs[i]))\n",
    "        \n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization parameters and compiling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initiate some parameters for TRAINING ##\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "\n",
    "# Shape for input\n",
    "n_num_features = X_train.loc[:, 'year':].shape[1]\n",
    "# Input shape for text data must be (1, ), stated in the model\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(vectorizer.get_vocabulary())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create and compile model instance\n",
    "# model = compile_model(\n",
    "#     get_uncompiled_model(\n",
    "#         output_dim=128, \n",
    "#         lstm_units=[32, 32, 32], \n",
    "#         dense_units=[16, 16, 16],\n",
    "#         dropout_rates=[0.3, 0.3, 0.3]\n",
    "#     ), \n",
    "#     learning_rate=0.01,\n",
    "# )\n",
    "\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING ROUND 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create checkpoint, this will autosave the entire model per epoch\\nIF the model achieves a better accuracy than the best so far...'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\"\"\"Create checkpoint, this will autosave the entire model per epoch\n",
    "IF the model achieves a better accuracy than the best so far...\"\"\"\n",
    "\n",
    "# checkpoint_path = \"/Users/kelstonchen/Documents/QueensMA/MA_Essay/MODELS/checkpoints/lstm_model-{epoch:02d}-{val_categorical_accuracy:.3f}\"\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     checkpoint_path, \n",
    "#     monitor='val_categorical_accuracy',\n",
    "#     verbose=1,\n",
    "#     save_best_only=True,\n",
    "#     mode='max',\n",
    "# )\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_categorical_accuracy', \n",
    "#     patience=5, \n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # Train model -- First Round.\n",
    "# model_hist = model.fit(\n",
    "#     {\"text\": X_train.loc[:, 'text'], \"num\": X_train.loc[:, 'year':]}, \n",
    "#     y_train_wide,\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_split=0.2,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     verbose=1,\n",
    "#     callbacks=[checkpoint, early_stopping]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " num (InputLayer)               [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " text (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 9)            19          ['num[0][0]']                    \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, None)        0           ['text[0][0]']                   \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 9)            0           ['normalization[0][0]']          \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 128)    762880      ['text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          1280        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " spatial_dropout1d (SpatialDrop  (None, None, 128)   0           ['embedding[0][0]']              \n",
      " out1D)                                                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 128)         512         ['dense[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, None, 128)    131584      ['spatial_dropout1d[0][0]']      \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 128)          0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, None, 128)   512         ['lstm[0][0]']                   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 128)          16512       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, None, 128)    131584      ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 128)         512         ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, None, 128)   512         ['lstm_1[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 128)          131584      ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64)           8256        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 128)         512         ['lstm_2[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64)          256         ['dense_2[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 192)          0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 3)            579         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,187,094\n",
      "Trainable params: 1,185,667\n",
      "Non-trainable params: 1,427\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Load Saved Model ##\n",
    "model = keras.models.load_model(\"/Users/kelstonchen/Documents/QueensMA/MA_Essay/MODELS/lstm_model-05-0.76\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 4s 22ms/step - loss: 0.6421 - categorical_accuracy: 0.7573 - auc: 0.8140 - precision: 0.7645 - recall: 0.7403\n"
     ]
    }
   ],
   "source": [
    "# Evaluate -- Test model\n",
    "loss, acc, auc, pre, recall = model.evaluate(\n",
    "    {\"text\": X_test.loc[:, 'text'], \"num\": X_test.loc[:, 'year':]}, \n",
    "    y_test_wide, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 5s 25ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.28      0.43       893\n",
      "           1       0.75      0.98      0.85      3408\n",
      "           2       0.77      0.39      0.51       894\n",
      "\n",
      "    accuracy                           0.76      5195\n",
      "   macro avg       0.82      0.55      0.60      5195\n",
      "weighted avg       0.78      0.76      0.72      5195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predicted labels\n",
    "pred_labels = get_predicted_labels(\n",
    "    get_predicted_probs(model)\n",
    ")\n",
    "print(metrics.classification_report(y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-TRAINED EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the vocabulary from our vectorizer, create a word index\n",
    "# Note: vectorizer was fitted to our data\n",
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = os.path.join(\n",
    "    os.path.expanduser(\"~\"), \n",
    "    \"Documents/QueensMA/MA_Essay/Data/glove.6B/glove.6B.200d.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_index(path):\n",
    "    embeddings_index = {}\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = load_embeddings_index(path_to_glove_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embedding_matrix(embedding_index, embedding_size, other_embeddings_index=None):\n",
    "    \"\"\"\n",
    "    The following grabs the embedding vector for each word found in our data\n",
    "    to the pre-trained embedding vector from GloVe.\n",
    "\n",
    "    The returned embedding matrix is one with the embedding vectors from GloVe.\n",
    "    \"\"\"\n",
    "\n",
    "    ## INITAL PARAMS FOR EMBEDDING MATRIX ##\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    # Prepare embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_size))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word) # get the embedding vector from GloVe\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            # This includes the representation for \"padding\" and \"OOV\"\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "\n",
    "        # NEW ADDITION ##\n",
    "        elif other_embeddings_index != None and embedding_vector is None:\n",
    "            embedding_matrix[i] = other_embeddings_index.get(word)\n",
    "            # print(word)\n",
    "        else:\n",
    "            misses += 1\n",
    "            # print(word)\n",
    "\n",
    "    print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 5273 words (686 misses)\n"
     ]
    }
   ],
   "source": [
    "## INITAL PARAMS FOR EMBEDDING MATRIX ##\n",
    "embedding_size = 200 # output dim - MUST match the dimension of pretrained embedding\n",
    "# Vocabulary size\n",
    "vocab_size = len(vectorizer.get_vocabulary())\n",
    "\n",
    "# Generate matrix\n",
    "embedding_matrix = gen_embedding_matrix(embedding_index=embeddings_index, embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create and compile model instance\n",
    "model_glove = compile_model(\n",
    "    get_uncompiled_model(\n",
    "        spatial_dropout_rate=0.1,\n",
    "        output_dim=embedding_size, \n",
    "        lstm_units=[128, 128, 128], \n",
    "        dense_units=[16, 16, 16],\n",
    "        dropout_rates=[0.1, 0.1, 0.1],\n",
    "        pre_trained_embedding=keras.initializers.Constant(embedding_matrix) # LOAD GloVe embeddings \n",
    "    ), \n",
    "    learning_rate=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING ROUND 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Create checkpoint, this will autosave the entire model per epoch\\nIF the model achieves a better accuracy than the best so far...'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\"\"\"Create checkpoint, this will autosave the entire model per epoch\n",
    "IF the model achieves a better accuracy than the best so far...\"\"\"\n",
    "\n",
    "# checkpoint_path = \"/Users/kelstonchen/Documents/QueensMA/MA_Essay/MODELS/checkpoints/glove_model-{epoch:02d}-{val_categorical_accuracy:.3f}\"\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     checkpoint_path, \n",
    "#     monitor='val_categorical_accuracy',\n",
    "#     verbose=1,\n",
    "#     save_best_only=True,\n",
    "#     mode='max',\n",
    "# )\n",
    "\n",
    "# # Train model -- First Round.\n",
    "# model_hist = model_glove.fit(\n",
    "#     {\"text\": X_train.loc[:, 'text'], \"num\": X_train.loc[:, 'year':]}, \n",
    "#     y_train_wide,\n",
    "#     epochs=EPOCHS,\n",
    "#     validation_split=0.2,\n",
    "#     batch_size=200,\n",
    "#     verbose=1,\n",
    "#     callbacks=[checkpoint]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " num (InputLayer)               [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " text (InputLayer)              [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 9)            19          ['num[0][0]']                    \n",
      "                                                                                                  \n",
      " text_vectorization (TextVector  (None, None)        0           ['text[0][0]']                   \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_84 (Dropout)           (None, 9)            0           ['normalization[0][0]']          \n",
      "                                                                                                  \n",
      " embedding_31 (Embedding)       (None, None, 200)    1191800     ['text_vectorization[0][0]']     \n",
      "                                                                                                  \n",
      " dense_112 (Dense)              (None, 32)           320         ['dropout_84[0][0]']             \n",
      "                                                                                                  \n",
      " spatial_dropout1d_28 (SpatialD  (None, None, 200)   0           ['embedding_31[0][0]']           \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 32)          128         ['dense_112[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " lstm_84 (LSTM)                 (None, None, 64)     67840       ['spatial_dropout1d_28[0][0]']   \n",
      "                                                                                                  \n",
      " dropout_85 (Dropout)           (None, 32)           0           ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, None, 64)    256         ['lstm_84[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dense_113 (Dense)              (None, 32)           1056        ['dropout_85[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_85 (LSTM)                 (None, None, 64)     33024       ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 32)          128         ['dense_113[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_163 (Batch  (None, None, 64)    256         ['lstm_85[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout_86 (Dropout)           (None, 32)           0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " lstm_86 (LSTM)                 (None, 32)           12416       ['batch_normalization_163[0][0]']\n",
      "                                                                                                  \n",
      " dense_114 (Dense)              (None, 16)           528         ['dropout_86[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_164 (Batch  (None, 32)          128         ['lstm_86[0][0]']                \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 16)          64          ['dense_114[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " concatenate_28 (Concatenate)   (None, 48)           0           ['batch_normalization_164[0][0]',\n",
      "                                                                  'batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " dense_115 (Dense)              (None, 3)            147         ['concatenate_28[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,308,110\n",
      "Trainable params: 115,811\n",
      "Non-trainable params: 1,192,299\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ## Load Saved Model ##\n",
    "model_glove = keras.models.load_model('/Users/kelstonchen/Documents/QueensMA/MA_Essay/MODELS/glove_model-16-0.793')\n",
    "model_glove.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 2s 10ms/step - loss: 0.6133 - categorical_accuracy: 0.7881 - auc_30: 0.8772 - precision_30: 0.7955 - recall_30: 0.7815\n"
     ]
    }
   ],
   "source": [
    "# Evaluate -- Test model\n",
    "loss, acc, auc, pre, recall = model_glove.evaluate(\n",
    "    {\"text\": X_test.loc[:, 'text'], \"num\": X_test.loc[:, 'year':]},\n",
    "    y_test_wide, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 2s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.757627</td>\n",
       "      <td>0.797043</td>\n",
       "      <td>0.758958</td>\n",
       "      <td>0.788065</td>\n",
       "      <td>0.771209</td>\n",
       "      <td>0.783714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.500560</td>\n",
       "      <td>0.933392</td>\n",
       "      <td>0.521253</td>\n",
       "      <td>0.788065</td>\n",
       "      <td>0.651735</td>\n",
       "      <td>0.788065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.602832</td>\n",
       "      <td>0.859846</td>\n",
       "      <td>0.618037</td>\n",
       "      <td>0.788065</td>\n",
       "      <td>0.693572</td>\n",
       "      <td>0.774054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>893.000000</td>\n",
       "      <td>3408.000000</td>\n",
       "      <td>894.000000</td>\n",
       "      <td>0.788065</td>\n",
       "      <td>5195.000000</td>\n",
       "      <td>5195.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             negative      neutral    positive  accuracy    macro avg  \\\n",
       "precision    0.757627     0.797043    0.758958  0.788065     0.771209   \n",
       "recall       0.500560     0.933392    0.521253  0.788065     0.651735   \n",
       "f1-score     0.602832     0.859846    0.618037  0.788065     0.693572   \n",
       "support    893.000000  3408.000000  894.000000  0.788065  5195.000000   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.783714  \n",
       "recall         0.788065  \n",
       "f1-score       0.774054  \n",
       "support     5195.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.877\n"
     ]
    }
   ],
   "source": [
    "# Get predicted labels\n",
    "pred_labels = get_predicted_labels(\n",
    "    get_predicted_probs(model_glove)\n",
    ")\n",
    "# print(metrics.classification_report(\n",
    "#     y_test, pred_labels,\n",
    "#     target_names=['negative', 'neutral', 'positive'], \n",
    "#     digits=3\n",
    "# )\n",
    "#      )\n",
    "report = pd.DataFrame(\n",
    "    metrics.classification_report(\n",
    "        y_test, pred_labels, \n",
    "        target_names=['negative', 'neutral', 'positive'], \n",
    "        digits=3,\n",
    "        output_dict=True\n",
    "    )\n",
    ")\n",
    "display(report)\n",
    "print(f\"AUC: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE REPORT ##\n",
    "report.to_csv(os.path.join(output_path, \"tables\", \"LSTM_report.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.651734908538086"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## BALANCED ACCURACY ##\n",
    "metrics.balanced_accuracy_score(y_test, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
